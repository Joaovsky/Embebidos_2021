%Initialize Q table
Q_table = [0 0 0;0 0 0;0 0 0;0 0 0;0 0 0;0 0 0;0 0 0];
%Randomize Q initial values
Q_table = randn(size(Q_table));
%Set initial parameters
init_cur_temp = 20;
Action = 1;
Gama = 0.9;
Epsilon = 0.8;
Learn_rate = 0.7;
Total_episodes = 100;
Episode_reward = 0;
max_q = 0;
%Loop for each episode
for episode=1:Total_episodes
%Initialize S
[cur_temp, cs, r] = environment(Action,init_cur_temp);
%Loop for each step of the episode
while(1)
%Choose A from S using policy derived from Q using 
probability = rand;
if rand < Epsilon
    [Chosen_value,Action] = max((Q_table(cs,:))');
else
    Action = randi(width(Q_table));
end
%Take action A, observe R, S'
[cur_temp, ns, r] = environment(Action,cur_temp);
Episode_reward = Episode_reward+r;
%Determine max Q value of given action and next state
[max_q] = max((Q_table(ns,:))');
%Update Q value from current state and action
Q_table(cs,Action) = (1-Learn_rate)*Q_table(cs,Action) + Learn_rate*(reward(cs,Action) + Gama*max_q);
%S <- S'
cs = ns;
%If S = 1, temperature converged, break
if cs == 1
    break;
end
end
hold on
plot(episode, Episode_reward/episode, 'c+', episode, cur_temp, 'o');
%plot(episode, cur_temp, 'o');
end
hold off
%Save QTable with otimized values
save('QTablev15.mat','Q_table');